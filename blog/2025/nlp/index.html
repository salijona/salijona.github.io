<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 1. The Limits of Adversarial Text Attacks | Salijona Dyrmishi </title> <meta name="author" content="Salijona Dyrmishi"> <meta name="description" content="When AI Gets Fooled but Humans Don’t."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://salijona.github.io/blog/2025/nlp/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Salijona </span> Dyrmishi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item"> <a class="nav-link" href="/about/"> About </a> </li> <li class="nav-item"> <a class="nav-link" href="/publications/"> Publications </a> </li> <li class="nav-item"> <a class="nav-link" href="/blog/"> Blog </a> </li> <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown-4" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"> Engagements </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown-4"> <a class="dropdown-item" href="/engagements/outreach/">SciComm</a> <a class="dropdown-item" href="/engagements/organization/">Organization</a> <a class="dropdown-item" href="/engagements/teaching/">Teaching</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">1. The Limits of Adversarial Text Attacks</h1> <h4 class="post-subtitle">When AI Gets Fooled but Humans Don’t.</h4> <p class="post-meta"> Created in September 05, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/adversarial-machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> adversarial machine learning</a>   <a href="/blog/tag/adversarial-attacks"> <i class="fa-solid fa-hashtag fa-sm"></i> adversarial attacks</a>   <a href="/blog/tag/ai-ml-security"> <i class="fa-solid fa-hashtag fa-sm"></i> AI/ML security</a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In the first post of this series, we introduced adversarial attacks as a security threat to AI/ML systems. We also discussed why realism matters when evaluating and defending against them. Attacks that succeed in simulations often fail in real-world settings, yet defining and reproducing “realistic” conditions is already difficult for many applications. In this post, we’re zooming in on natural language processing (NLP) to see why these challenges are even harder when the medium is language.</p> <p>For image-based adversarial attacks that we have seen so far, realism is often defined by imperceptibility. Mathematically, this is captured by minimizing the distance between the original and adversarial image using Lp norms. For example, the L1 norm sums up all the small differences in pixel values compared to the original image. Smaller distances mean the images look almost identical to the human eye. The goal is to make changes so subtle that people don’t notice them, while still fooling the model. With text, however, things are much trickier. A tiny change, such as swapping “movie” for “mov1e” might deceive a classifier, but it is immediately obvious to any human reader.</p> <p>This disconnect matters because, in many NLP applications, humans remain part of the decision-making loop. A phishing email with awkward phrasing probably won’t get clicked, even if a sophisticated AI/ML filter fails to catch it. A fake news headline that “feels off” will raise suspicion, even if it bypasses automated detection. Offensive language that appears altered won’t convince moderators, even if the model is fooled.</p> <p>In these scenarios, an attack is only truly dangerous if it can deceive both algorithms and humans. This means that creating a realistic and effective adversarial example requires thinking about how humans interpret text, not just how models process it.</p> <blockquote> <p>Attack realism in text-based use cases, can be viewed as a spectrum.</p> </blockquote> <p>At one end are unrealistic attacks that are easy for humans to detect, but may still confuse automated models. In the middle are somewhat realistic attacks that sound plausible but still raise suspicion. At the far end are highly realistic attacks designed to deceive both humans and AI/ML systems.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/series/text_spectrum-480.webp 480w,/assets/img/series/text_spectrum-800.webp 800w,/assets/img/series/text_spectrum-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/series/text_spectrum.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>To illustrate, consider the phishing examples in the figure:</p> <p><strong>Unrealistic (toy attack):</strong><br> “You lottery got…!! Links wkor ///”<br> These attacks are nonsensical and grammatically broken. Humans would easily dismiss them, but some detection models might still misclassify them.</p> <p><strong>Somewhat realistic (model fooled, humans suspicious):</strong><br> “You have win the lottery. Click here claim your prize.”<br> The message is understandable but clunky and unpolished. Many automated filters might fail to flag it, yet most users would still hesitate to trust it.</p> <p><strong>Highly realistic (true threat):</strong><br> “You have won the lottery. Click this link to get your money.”<br> This represents a serious security risk. The message is fluent, grammatically correct, semantically coherent, and persuasive enough to bypass both detection systems and human judgment.</p> <p>Real attackers aim for highly realistic adversarial text to maximize success while avoiding detection. Consequently, most NLP robustness tests try to preserve meaning, grammar, and natural style so the attacks resemble real-world scenarios. However, these methods often rely on mathematical proxies that miss the subtle cues humans use to judge whether a message is suspicious. Our survey of the literature shows that human perception is rarely included in evaluations: 3 of 16 studies involved no human participants, and those that did often rely on just a handful of people (1–3) assessing narrow aspects like meaning preservation or label consistency. Such limited checks overlook the broader ways humans detect suspicious text.</p> <p>To address this, we conducted one of the first large-scale studies on human perception of adversarial text, surveying 378 participants across nine state-of-the-art attack methods to see how people actually respond. We focused on two key aspects of how humans perceived the adversarial texts:</p> <p><strong>Validity:</strong> Does the text preserve its original intent after being manipulated?<br> For example, if a hateful message like “I hate all XXX” is changed to “I love XXX,” the model might be fooled, but the attacker’s goal fails because the meaning that reaches the end user has been completely reversed.</p> <p><strong>Naturalness:</strong> Does the text appear manipulated or computer-generated to a human reader?<br> Even if an adversarial text is valid, one that feels awkward, unnatural, or obviously altered may fail to convince people.</p> <p>The results showed that what fools models often does not fool humans. We explore these findings and their implications in our ACL paper, “How Do Humans Perceive Adversarial Text? A Reality Check on the Validity and Naturalness of Word-Based Adversarial Attacks.”</p> <p><strong>Validity:</strong> Humans correctly understood the original intent of about 72% of adversarial texts, compared to nearly 89% for the original texts. This shows that many attacks produce a significant portion of invalid adversarial examples.</p> <p><strong>Naturalness:</strong> Adversarial texts also stood out as unnatural. Over 60% of participants flagged them as likely computer-generated, compared to just 21% for the original texts. When asked to pinpoint the changes, participants identified the altered words in roughly 50% of the cases. This shows that while adversarial examples may fool models, they often fail the human “readability and believability” test.</p> <p>These results show that even the most advanced attacks today often wouldn’t succeed in real-world settings where humans are part of the system. This is relevant for cases like content moderation, or misinformation control, where human judgment remains a critical part of the system, as we discussed in the introduction.</p> <p>Out of curiosity, we also explored why certain texts trigger human suspicion. From there, we could observe the following trends:</p> <ul> <li> <strong>Clear meaning matters:</strong> Texts that preserved their original intent and were easy to understand were generally seen as more natural, while messages with unclear or distorted meaning were much more likely to be flagged as “computer-generated.”</li> <li> <strong>Grammar errors, not as much:</strong> Minor grammar mistakes didn’t necessarily make a text feel unnatural if the meaning was clear. But once a message became awkward or nonsensical, it immediately stood out to readers.</li> </ul> <p>These were the main results, and for full details you can check out our paper. Few were the main lessons, from this large-scale human study. It became clear that human psychology plays a major role in whether an attack succeeds. Focusing only on model performance misses an important point: human judgment gives a more realistic perspective for many applications. Future research on adversarial testing in NLP should focus on crafting examples that are semantically valid, natural, and persuasive, while keeping humans in the loop to accurately assess real-world threat potential.</p> <p><em>A small disclaimer</em>: Our study predates the recent surge in Large Language Models (LLMs). These systems generate fluent, convincing text that may appear far less suspicious, posing new challenges for adversarial NLP research.</p> <hr> <p><strong>Note:</strong> <em>This blog series is based on research about the realism of current adversarial attacks from my time at the SerVal group, SnT (University of Luxembourg). It’s an easy-to-digest format for anyone interested in the topic, especially those who may not have time (or willingness) to read our full papers.</em></p> <p><em>The work and results presented here are a team effort, including <a href="https://maxcordy.github.io/" rel="external nofollow noopener" target="_blank">Asst. Prof. Dr. Maxime Cordy</a>, <a href="https://scholar.google.com/citations?user=4RhGnOoAAAAJ&amp;hl=en&amp;oi=ao" rel="external nofollow noopener" target="_blank">Dr. Thibault Simonetto</a>, <a href="https://scholar.google.com/citations?user=UcvKgR0AAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Dr. Salah Ghamizi</a>, <a href="https://scholar.google.com/citations?user=KcGsVdIAAAAJ&amp;hl=fr&amp;oi=ao" rel="external nofollow noopener" target="_blank">(to be Dr.) Mohamed Djilani</a>, <a href="https://mihaela-stoian.github.io/" rel="external nofollow noopener" target="_blank">(to be Dr.) Mihaela C. Stoian</a> and <a href="https://egiunchiglia.github.io/" rel="external nofollow noopener" target="_blank">Asst. Prof. Dr. Eleonora Giunchiglia</a>.</em></p> <p><em>If you want to dig deeper into the results or specific subtopics, check out the papers linked in each blog post.</em></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-2xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/tell-me-a-story-2-reflections-after-five-years-in-academia/">Tell me a story #2 - Reflections After Five Years in Academia</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/intro/">0. AI/ML Security Is Broken Without Realistic Adversarial Testing</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/slr/">The Forgotten Domain: Adversarial Machine Learning for Tabular Data</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Salijona Dyrmishi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>