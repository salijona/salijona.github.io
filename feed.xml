<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://salijona.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://salijona.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-08T20:30:13+00:00</updated><id>https://salijona.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">3. From Testing to Defenses: When Realistic Attacks Help</title><link href="https://salijona.github.io/blog/2025/hardening/" rel="alternate" type="text/html" title="3. From Testing to Defenses: When Realistic Attacks Help"/><published>2025-09-08T00:00:00+00:00</published><updated>2025-09-08T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/hardening</id><content type="html" xml:base="https://salijona.github.io/blog/2025/hardening/"><![CDATA[<p>Finding the attacks that teach models to defend themselves.</p> <p>So far in this series, we’ve seen how adversarial testing often relies on unrealistic attacks, giving AI developers a false sense of security. What testing really provides is a map of weaknesses. It shows where the shield has cracks. The real challenge comes next: hardening those weak points, so the shield can withstand real-world attacks. As researchers and practitioners, we must remember that testing is only a tool. The ultimate goal is defense.</p> <p>Hardening means repeatedly exposing your model to attacks and forcing it to adapt. You generate adversarial examples, retrain the model using the true or intended labels for those examples, test again, and repeat the cycle. Over time, the model learns to resist the tricks attackers are likely to use. In practice, this process goes by different names depending on technical variations, such as adversarial training, retraining, or fine-tuning. Regardless of the name, the goal is always the same: to expose the model to hostile conditions so it is prepared for real-world attacks.</p> <p>In the figure below, left side shows a malware detector identifying malware files. We then create adversarial examples, shown as the two files with arrows, that exploit weaknesses in the model’s decision boundary. In right side, the model has been retrained using these adversarial samples, which updates the decision boundary. The model can now correctly detect the files that previously slipped through. While the figure uses only two examples for illustration, in practice retraining would involve thousands or even hundreds of thousands of samples to make the model significantly more robust.</p> <p float="left"> <img src="assets/img/series/unhardened_model.png" alt="Image 1" width="45%"/> <img src="assets/img/series/hardened_model.png" alt="Image 2" width="45%"/> </p> <p><em>Figure: (Left) A malware detector before adversarial hardening. (Right) The same model after retraining on adversarial examples, updating its decision boundary to correctly detect previously missed files.</em></p> <p>But here’s the key question:</p> <blockquote> <p>Does the realism of adversarial examples used in hardening matter?</p> </blockquote> <p>Consider the malware detector above. An unrealistic attack might modify a malicious file in ways that break it. Even if the detector lets it run, the attack fails because the file cannot execute. A realistic attack, by contrast, carefully modifies the malware to evade detection while still achieving its malicious goal, such as locking the computer, making it much harder to stop. If the model is trained only on unrealistic examples, it may still fail against realistic ones. This uncertainty is exactly what our study set out to investigate in <em><a href="https://ieeexplore.ieee.org/document/10179316">On the Empirical Effectiveness of Unrealistic Adversarial Hardening Against Realistic Adversarial Attacks</a></em>.</p> <p>Generating unrealistic attacks is fast and inexpensive because they can be created without worrying about whether the file will execute. Crafting realistic attacks, by contrast, is far more costly in both computation and engineering effort. Modifying malware to evade detection requires carefully changing executable files without breaking them and testing them in a specialized sandbox to ensure they still carry out their malicious purpose (e.g., asking for ransom). This process can take tens of thousands of times longer than simply altering a few bytes at random, which can take days or even weeks. If unrealistic examples can expose the same weak points in the model’s decision boundary as realistic attacks, they could serve as effective stand-ins, potentially saving substantial time and resources in the hardening process.</p> <p float="left"> <img src="assets/img/series/unrealistic_example.png" alt="Image 1" width="45%"/> <img src="assets/img/series/realistic_example.png" alt="Image 2" width="45%"/> </p> <p><em>Figure: Adversarial examples used to harden the model (Left) A malicious file that does not execute. (Right) A malicious file that infects the computer and asks for ransom.</em></p> <p>We hoped that unrealistic attacks could help harden models against real-world threats, so we tested the impact of unrealistic attacks alongside realistic ones across three distinct domains: text classification, botnet detection, and malware detection, each presenting unique challenges.</p> <ul> <li> <p><strong>Text classification:</strong> Simple tricks like character swaps provided only modest improvements in robustness, while attacks that preserved meaning, such as swapping words with synonyms, were far more effective.</p> </li> <li> <p><strong>Botnet detection:</strong> In this domain, the model’s vulnerabilities aligned closely with even simple, unrealistic attacks, making the hardening process more forgiving.</p> </li> <li> <p><strong>Malware detection:</strong> This proved the toughest case. Unrealistic attacks failed entirely because they never reached the model’s true blind spots, leaving the system exposed.</p> </li> </ul> <p>Across these domains, it became clear that the effectiveness of unrealistic hardening depends on how closely attacks target the model’s critical weaknesses.</p> <p>We also tried increasing the number of unrealistic examples used in the hardening process for malware and botnets to see if it was simply a matter of quantity, but the results did not change.</p> <p>These results show that robustness isn’t about generating more examples, but about generating the right ones. Unrealistic attacks only help when they push the model in the same direction and touch the same critical features as real attacks. In our study, this alignment occurred for botnets, where even cheap, simple attacks nudged the model toward real robustness. For malware, however, the alignment didn’t exist. No matter how many unrealistic examples we produced, the models remained exposed.</p> <p>Looking at these outcomes, it’s easy to see the next step. First, a model trained on unrealistic attacks may seem secure, but real robustness is measured by its response to realistic, unseen threats. Second, even small sets of realistic adversarial examples can be extremely valuable, acting like a compass to guide cheaper, easier-to-generate attacks toward the most critical vulnerabilities. Instead of relying solely on toy attacks for convenience, we should focus on shaping them to reflect the characteristics of real-world threats.</p> <p>The stakes of ignoring realism in adversarial hardening are high. A model that seems robust in the lab may still fail catastrophically when faced with real-world attacks, leaving organizations vulnerable to malware, spam campaigns, or network intrusions. Overconfidence in lab-tested models can lead to wasted resources, false security, and even regulatory or reputational consequences if attacks succeed. By focusing on realistic adversarial examples, even a small carefully curated set, we can guide cheaper, simpler attacks toward the model’s true weaknesses, ensuring that hardening translates into real-world resilience. In short, as we emphasized realism in testing, we must demand it in hardening.</p> <h3 id="a-note-on-scope-and-generalization">A note on scope and generalization:</h3> <p>When presenting results by domain (text classification, botnet detection, and malware detection), we used a fixed model architecture, a fixed hardening strategy, and a fixed feature representation for each domain. As a result, the findings within each domain may not generalize to other architectures, hardening methods, or feature sets. Future work could explore more expansive settings, similar to the approach taken by Bostani et al. in <em>“On the Effectiveness of Adversarial Training on Malware Classifiers.”</em></p> <hr/> <p><strong>Note:</strong> <em>This blog series is based on research about the realism of current adversarial attacks from my time at the SerVal group, SnT (University of Luxembourg). It’s an easy-to-digest format for anyone interested in the topic, especially those who may not have time (or willingness) to read our full papers.</em></p> <p><em>The work and results presented here are a team effort, including <a href="https://maxcordy.github.io/">Asst. Prof. Dr. Maxime Cordy</a>, <a href="https://scholar.google.com/citations?user=4RhGnOoAAAAJ&amp;hl=en&amp;oi=ao">Dr. Thibault Simonetto</a>, <a href="https://scholar.google.com/citations?user=UcvKgR0AAAAJ&amp;hl=fr">Dr. Salah Ghamizi</a>, <a href="https://scholar.google.com/citations?user=KcGsVdIAAAAJ&amp;hl=fr&amp;oi=ao">(to be Dr.) Mohamed Djilani</a>, <a href="https://mihaela-stoian.github.io/">(to be Dr.) Mihaela C. Stoian</a> and <a href="https://egiunchiglia.github.io/">Asst. Prof. Dr. Eleonora Giunchiglia</a>.</em></p> <p><em>If you want to dig deeper into the results or specific subtopics, check out the papers linked in each blog post.</em></p>]]></content><author><name></name></author><category term="research"/><category term="adversarial machine learning"/><category term="adversarial attacks"/><category term="AI/ML security"/><summary type="html"><![CDATA[Finding the attacks that teach models to defend themselves.]]></summary></entry><entry><title type="html">2b. Constrained Adversarial DGMs (C-AdvDGMs): Realistic Attacks with Generative Models</title><link href="https://salijona.github.io/blog/2025/tabB/" rel="alternate" type="text/html" title="2b. Constrained Adversarial DGMs (C-AdvDGMs): Realistic Attacks with Generative Models"/><published>2025-09-08T00:00:00+00:00</published><updated>2025-09-08T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/tabB</id><content type="html" xml:base="https://salijona.github.io/blog/2025/tabB/"><![CDATA[<p>In our previous post, we introduced Constrained Deep Generative Models (C-DGMs), which were designed to generate tabular data that not only looks realistic but also respects the rules of the domain. While this was an important step toward producing trustworthy synthetic data, it naturally led to a new question: can these models be used to probe the weaknesses of tabular ML systems?</p> <p>In this post, we explore exactly that. By extending C-DGMs to generate adversarial examples, we arrive at Constrained Adversarial DGMs (C-AdvDGMs). By enforcing domain constraints, these examples remain valid and realistic, reflecting the types of inputs a model might encounter in practice and making the attacks practically relevant. This approach is described in our work <em><a href="https://arxiv.org/abs/2409.12642">Deep Generative Models as an Adversarial Attack Strategy for Tabular Machine Learning</a></em>, which presents the methods behind C-AdvDGMs.</p> <p><strong>Recap.</strong> Constrained Deep Generative Models (C-DGMs) integrate a Constraint Layer (CL) that enforces domain rules through a repair strategy. Given the constraints and feature ordering, the CL minimally adjusts any generated samples that violate the rules, ensuring they are valid while preserving the underlying data distribution. For example, it guarantees relationships like total_income = salary + bonus + other components are maintained.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/series/cadvdgm-480.webp 480w,/assets/img/series/cadvdgm-800.webp 800w,/assets/img/series/cadvdgm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/series/cadvdgm.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The figure above illustrates the overall framework of C-AdvDGMs. The main modifications from a standard DGM are highlighted by the red numbers:</p> <ol> <li> <p><strong>Constraint-Aware Generation from Initial Input.</strong> Instead of creating data from scratch, the generator takes an existing sample and applies small, targeted perturbations. The constraint layer ensures the output follows all domain rules, keeping samples valid and realistic.</p> </li> <li> <p><strong>Target Model Evaluation.</strong> In C-AdvDGMs, the generated examples are additionally tested on a target classifier to evaluate how effectively they fool the model, a step that is not present in standard DGMs.</p> </li> <li> <p><strong>Optimized Training.</strong> The initial C-AdvDGM loss is extended into a combined loss function that balances four objectives: preserving data realism, maximizing adversarial success, controlling perturbation size, and enforcing user-defined constraints.</p> </li> </ol> <p>To revisit our initial question, we wanted to see if C-AdvDGMs could expose weaknesses in tabular ML systems.</p> <p><strong>What we discovered:</strong> Across the four types we tested, performance varied quite a bit. Only one model managed a competitive adversarial success rate (ASR), and even it was often outperformed by gradient-based and search-based attacks, ranking second at best. What really stood out was the role of the Constraint Layer (CL). Including it during training or sampling consistently boosted success rates, showing just how important it is to enforce domain knowledge when generating realistic adversarial examples.</p> <p>C-AdvDGMs introduce a novel way to generate realistic adversarial examples by extending deep generative models with constraint-aware generation. While they do not always match traditional attacks in terms of raw success rates, they produce adversarial samples that are valid, plausible, and aligned with real-world domain constraints. At the same time, C-AdvDGMs represent just one path toward realistic adversarial attacks. Alternative approaches such as adapting gradient-based, gradient-free, or learning-based attacks to respect feasibility constraints remain promising directions for improving overall attack strength.</p> <p>By combining constraint-aware generation with advances in other adversarial techniques, researchers can build a more comprehensive toolkit for adversarial testing. This opens the door to more reliable robustness evaluations for tabular ML systems, particularly in high-stakes tabular domains like finance, healthcare, and security.</p> <hr/> <p><strong>Note:</strong> <em>This blog series is based on research about the realism of current adversarial attacks from my time at the SerVal group, SnT (University of Luxembourg). It’s an easy-to-digest format for anyone interested in the topic, especially those who may not have time (or willingness) to read our full papers.</em></p> <p><em>The work and results presented here are a team effort, including <a href="https://maxcordy.github.io/">Asst. Prof. Dr. Maxime Cordy</a>, <a href="https://scholar.google.com/citations?user=4RhGnOoAAAAJ&amp;hl=en&amp;oi=ao">Dr. Thibault Simonetto</a>, <a href="https://scholar.google.com/citations?user=UcvKgR0AAAAJ&amp;hl=fr">Dr. Salah Ghamizi</a>, <a href="https://scholar.google.com/citations?user=KcGsVdIAAAAJ&amp;hl=fr&amp;oi=ao">(to be Dr.) Mohamed Djilani</a>, <a href="https://mihaela-stoian.github.io/">(to be Dr.) Mihaela C. Stoian</a> and <a href="https://egiunchiglia.github.io/">Asst. Prof. Dr. Eleonora Giunchiglia</a>.</em></p> <p><em>If you want to dig deeper into the results or specific subtopics, check out the papers linked in each blog post.</em></p>]]></content><author><name></name></author><category term="research"/><category term="adversarial machine learning"/><category term="adversarial attacks"/><category term="AI/ML security"/><summary type="html"><![CDATA[Extending C-DGMs to Safely Test Model Vulnerabilities]]></summary></entry><entry><title type="html">2a. From Deep Generative Models (DGMs) to Constrained Deep Generative Models (C-DGMs)</title><link href="https://salijona.github.io/blog/2025/tabA/" rel="alternate" type="text/html" title="2a. From Deep Generative Models (DGMs) to Constrained Deep Generative Models (C-DGMs)"/><published>2025-09-07T00:00:00+00:00</published><updated>2025-09-07T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/tabA</id><content type="html" xml:base="https://salijona.github.io/blog/2025/tabA/"><![CDATA[<p>In the last post, we explored why adversarial examples for tabular data must be realistic. A negative age, a bank account with inconsistent balances, or a medical record with incompatible diagnoses may succeed in bypassing a model, but they are meaningless in practice. They belong to a space of points that cannot exist, and as such they represent useless threats. Our goal is not to discover these impossible cases, but to identify the ones that truly matter. The realistic adversarial examples that expose vulnerabilities models will face in the real world.</p> <p>Deep Generative Models (DGMs) such as GANs, VAEs, and diffusion models seem like a natural candidate for this task. They’re powerful and efficient, capable of learning complex data distributions and generating synthetic data that closely resembles real samples. This makes them a practical option for producing adversarial examples at scale.</p> <p>But here lies the problem:</p> <blockquote> <p>Statistically plausible does not necessarily mean “feasible”.</p> </blockquote> <p>When we tested popular tabular DGMs on six real-world datasets, we ran into some challenges. A large portion of the generated samples (sometimes more than half) failed to meet even the most basic feasibility constraints. In some datasets, nearly every generated record was invalid, with violation rates reaching as high as 95 to 100%. In theory, you could try filtering out invalid outputs by repeatedly generating new ones until you get a valid sample. This technique is called rejection sampling, but it stops being practical if most samples are rejected.</p> <p>The problem is straightforward: a model that merely captures statistical plausibility is not enough. To be useful for adversarial testing, or even for generating synthetic data more broadly, we need generative models that are realistic by design. That means models that respect the rules of the domain, not after the fact through rejection sampling, but as an integral part of the generation process.</p> <h2 id="the-constraint-layer-adding-a-rulebook-to-deep-generative-models">The Constraint Layer: Adding a Rulebook to Deep Generative Models</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/series/cdgm-480.webp 480w,/assets/img/series/cdgm-800.webp 800w,/assets/img/series/cdgm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/series/cdgm.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Before DGMs can be used for adversarial testing, they must first produce data that makes sense. That is where the Constraint Layer (CL) comes in. Inspired by neuro-symbolic AI, the CL acts as a rulebook embedded inside the generator. Its job is simple but crucial: check each generated record against a set of user-defined constraints, and if any rules are violated, nudge the values just enough to bring the record back into the feasible region. The correction is always minimal, so the repaired record stays as close as possible to what the generator originally produced.</p> <p>Constraints can capture simple rules, like ensuring <code class="language-plaintext highlighter-rouge">age ≥ 0</code>, or more complex structural relationships, such as <code class="language-plaintext highlighter-rouge">total_income = salary + bonus + other</code>. By enforcing these user-defined linear constraints directly during generation, the CL ensures that every output respects the domain’s ground truths rather than producing “plausible but impossible” data.</p> <p>An important detail is the order in which features are repaired, which is also specified by the user. Since features often depend on one another, the CL processes them sequentially according to this ordering. The sequence can follow the feature indices, be randomized, or be guided by causal or logical relationships in the data. Choosing the order carefully helps the CL make minimal adjustments while preserving the integrity of the generator’s learned distribution.</p> <hr/> <h4 id="example-how-feature-ordering-matters">Example: How Feature Ordering Matters</h4> <p>Consider a dataset with three features: <code class="language-plaintext highlighter-rouge">salary</code>, <code class="language-plaintext highlighter-rouge">bonus</code>, and <code class="language-plaintext highlighter-rouge">total_income</code>, with the linear constraint <code class="language-plaintext highlighter-rouge">total_income = salary + bonus</code>.</p> <p>Suppose the generator produces a sample where: <code class="language-plaintext highlighter-rouge">salary = 50 ^ bonus = 20 ^ total_income = 60</code>. The constraint is violated because <code class="language-plaintext highlighter-rouge">50 + 20 ≠ 60</code>.</p> <p>The constraint layer repairs features sequentially based on the user-specified ordering:</p> <ul> <li> <p><strong>Order 1: salary → bonus → total_income</strong><br/> The CL first checks <code class="language-plaintext highlighter-rouge">salary (50)</code> and <code class="language-plaintext highlighter-rouge">bonus (20)</code>, then adjusts <code class="language-plaintext highlighter-rouge">total_income</code> to <code class="language-plaintext highlighter-rouge">70</code>.</p> </li> <li> <p><strong>Order 2: total_income → salary → bonus</strong><br/> The CL first adjusts <code class="language-plaintext highlighter-rouge">total_income</code> to <code class="language-plaintext highlighter-rouge">70</code>, then checks <code class="language-plaintext highlighter-rouge">salary</code> and <code class="language-plaintext highlighter-rouge">bonus</code> (both fine).</p> </li> </ul> <p>In this simple example, both orders produce valid outputs, but in complex datasets with interdependent features, the ordering affects which features are minimally changed and how closely the repaired sample matches the original generator output. Ordering based on causal or logical relationships helps preserve the generator’s intended distribution.</p> <p>Because the CL is differentiable, it can be integrated into the training phase, so the generator gradually learns to produce valid samples directly. It can also act as a guardrail at inference time, repairing invalid outputs even for pre-trained or black-box models.</p> <p>With this addition, ordinary DGMs become <strong>Constrained DGMs (C-DGMs)</strong>, generators that not only capture statistical patterns but also obey the rules of the real world. This foundation sets the stage for the next step: extending C-DGMs into adversarial generators capable of producing realistic, constraint-respecting attacks.</p> <hr/> <p>But before we go there, let’s look at what we observed.</p> <ul> <li><strong>Constraint violations.</strong> With the constraint layer in place, feasibility violations dropped to zero. Unlike standard DGMs, which often produced invalid or inconsistent tabular records, C-DGMs generate samples that consistently respect domain rules. This makes them suitable for adversarial testing.</li> </ul> <p>However, the benefits of constrained generation go beyond adversarial testing. By ensuring that generated data is valid and realistic, C-DGMs can support a variety of practical applications: they enable the creation of privacy-preserving synthetic datasets, facilitate fairness auditing through plausible counterfactuals, provide useful data augmentation for small or imbalanced datasets, and generate reliable test cases for validating machine learning systems in safety-critical domains such as healthcare and finance.</p> <ul> <li> <p><strong>Utility.</strong> As a side experiment, to prove the benefits of C-DGMs beyond adversarial testing and to evaluate the overall quality of the synthetic data, we trained downstream classifiers on the generated samples. Models trained on constrained data outperformed those trained on unconstrained samples by up to 6.5% on utility metrics, demonstrating that the data is not only valid but also informative.</p> </li> <li> <p><strong>Runtime.</strong> Remarkably, these improvements came at minimal computational cost. Generating 1,000 samples with the constraint layer added only a few hundredths of a second, indicating that constrained generation can scale efficiently to larger datasets.</p> </li> </ul> <p>These results, along with additional findings and more detailed explanations of the construction of C-DGMs, are described further in <em><a href="https://arxiv.org/abs/2402.04823">How Realistic is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data</a></em>.</p> <p>Now with the foundation of C-DGMs in place, the next step is to explore adversarial generation. If C-DGMs can reliably generate valid samples, can they also generate adversarial ones that are both realistic and effective? That’s the challenge we’ll tackle in the next post.</p> <hr/> <p><strong>Note:</strong> <em>This blog series is based on research about the realism of current adversarial attacks from my time at the SerVal group, SnT (University of Luxembourg). It’s an easy-to-digest format for anyone interested in the topic, especially those who may not have time (or willingness) to read our full papers.</em></p> <p><em>The work and results presented here are a team effort, including <a href="https://maxcordy.github.io/">Asst. Prof. Dr. Maxime Cordy</a>, <a href="https://scholar.google.com/citations?user=4RhGnOoAAAAJ&amp;hl=en&amp;oi=ao">Dr. Thibault Simonetto</a>, <a href="https://scholar.google.com/citations?user=UcvKgR0AAAAJ&amp;hl=fr">Dr. Salah Ghamizi</a>, <a href="https://scholar.google.com/citations?user=KcGsVdIAAAAJ&amp;hl=fr&amp;oi=ao">(to be Dr.) Mohamed Djilani</a>, <a href="https://mihaela-stoian.github.io/">(to be Dr.) Mihaela C. Stoian</a> and <a href="https://egiunchiglia.github.io/">Asst. Prof. Dr. Eleonora Giunchiglia</a>.</em></p> <p><em>If you want to dig deeper into the results or specific subtopics, check out the papers linked in each blog post.</em></p>]]></content><author><name></name></author><category term="research"/><category term="adversarial machine learning"/><category term="adversarial attacks"/><category term="AI/ML security"/><summary type="html"><![CDATA[How to Make Synthetic Tabular Data Realistic by Design]]></summary></entry><entry><title type="html">2. Realistic Adversarial Attacks for Tabular Data Using Generative Models</title><link href="https://salijona.github.io/blog/2025/tabular/" rel="alternate" type="text/html" title="2. Realistic Adversarial Attacks for Tabular Data Using Generative Models"/><published>2025-09-06T00:00:00+00:00</published><updated>2025-09-06T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/tabular</id><content type="html" xml:base="https://salijona.github.io/blog/2025/tabular/"><![CDATA[<p>In our previous posts, we highlighted how adversarial attacks designed in controlled environments often fail to reflect real-world scenarios. So far, we have focused on images and text. In this post we will move to tabular data, allegedly one of the most widely used formats in practice, powering critical systems such as financial decision-making, medical diagnoses, and automated risk assessments. Tabular AI/ML applications are equally vulnerable to adversarial attacks. Here, we will explore how to construct adversarial attacks specifically for tabular data, with a focus on maintaining realism and practical relevance.</p> <p>Consider a credit card fraud detection system that processes records like this:</p> <table border="1"> <caption><strong>Original Transaction (Labelled as Fradulent)</strong></caption> <tr> <th>Age</th> <th>Income</th> <th>Transaction Amount</th> <th>Account Type</th> </tr> <tr> <td>35</td> <td>4000</td> <td>500</td> <td>Premium</td> </tr> </table> <p>This is a normal transaction that the system flags as suspicious. A naive attacker might think they can bypass the system simply by changing a single value. For example, they could inflate the transaction amount to an absurd number:</p> <p></p> <table border="1" style="border-collapse: collapse;"> <caption><strong>Adversarial Transaction (Labelled as Normal)</strong></caption> <tr> <th>Age</th> <th>Income</th> <th>Transaction Amount</th> <th>Account Type</th> </tr> <tr> <td>35</td> <td>4000</td> <td style="color:red">999,999,999</td> <td>Premium</td> </tr> </table> <p>Technically, the database accepts it, and it might fool an AI/ML model. But in reality, such an extreme number would immediately raise red flags: auditors, internal checks, or sanity rules would catch it instantly. This simple example shows that adversarial attacks on tabular ML models must respect constraints that make them plausible in the real world. We can identify three such constraints:</p> <ul> <li> <p><strong>Mutability Constraints</strong><br/> Certain features, such as blood type or date of birth, cannot be altered.</p> </li> <li> <p><strong>Structural Constraints</strong><br/> Tabular features have predefined domains. For example, age is restricted to integers between 0–120, and categorical variables can only take on a fixed set of values.</p> </li> <li> <p><strong>Inter-feature Dependencies</strong><br/> Features often have logical or statistical relationships. For instance, LoanAmount should not exceed a reasonable percentage of Income. These dependencies must be preserved.</p> </li> </ul> <p>Violating any of these constraints produces unrealistic or easily detected adversarial examples. Traditional adversarial attacks, which work well for images, often fail when applied in tabular settings because they do not account for mutability, structural limits, or inter-feature dependencies<sup>[1]</sup>. As a result, these methods frequently generate invalid or implausible records.</p> <p>Therefore, we need new adversarial approaches that can bypass machine learning models and at the same time respecting the inherent constraints of real-world tabular data. By ensuring that adversarial examples remain plausible and stealthy, these methods can more accurately simulate attacks that might occur in practice.</p> <p>We present below the main steps of our approach, and in the next two posts we will discuss them more in-depth.</p> <h2 id="our-approach-step-by-step">Our Approach: Step by Step</h2> <p>Building realistic adversarial attacks for tabular data can be approached in multiple ways. One option, which is the most generalizable, is to adapt existing image-based methods (e.g gradient-based, gradient-free, or learning-based attacks) by incorporating the feasibility constraints discussed earlier. This process involves masking immutable features, keeping values within valid ranges through rounding, for example, and applying post-hoc repair operations to maintain inter-feature dependencies. The second option is to develop entirely new techniques tailored to a specific use case, such as fraud detection. While potentially more effective for a single domain, this approach is difficult to generalize to other applications without significant effort.</p> <p>We opted for the first approach, combining generative models with feasibility constraints enforced through post-hoc repair. Generative models are particularly appealing because they are far more efficient than iterative optimization methods. Once trained, they can produce adversarial examples in a single step. Post-hoc repair ensures that all generated outputs satisfy the necessary constraints, providing guarantees that constraint-aware optimization alone may not offer.</p> <p>There are two key steps that we followed to build an adversarial attack for tabular ML which generates feasible input that are closer to a real-world scenario.</p> <ul> <li><strong>Step 1: Converting Deep Generative Models (DGMs) into Constrained DGMs (C-DGMs)</strong></li> </ul> <p>The first step is to ensure that generative models produce valid tabular data. To achieve this, we designed a differentiable constraint layer that performs post-hoc repair to correct outputs violating domain constraints, such as logical relationships between features (e.g., total_income = salary + bonus + other). This layer works as a filter that catches mistakes and fixes them. This results in a model capable of generating realistic tabular samples that conform to all feasibility requirements. This method is discussed in detail in our work <em><a href="https://arxiv.org/abs/2402.04823">How Realistic is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data</a></em>.</p> <ul> <li><strong>Step 2: Converting C-DGMs into Adversarial C-DGMs (C-AdvDGMs)</strong></li> </ul> <p>Once the constrained generative model reliably produces valid samples, we adapt it to generate adversarial examples that can mislead machine learning models. This is achieved through the incorporation of an adversarial loss and architectural modifications. This approach is described further in <em><a href="https://arxiv.org/abs/2409.12642">Deep Generative Models as an Adversarial Attack Strategy for Tabular Machine Learning</a></em>.</p> <p>In the next posts, we will take a closer look at these two steps. First, in Step 1, we will show how deep generative models (DGMs) can be transformed into constrained generative models (C-DGMs) that produce valid, realistic tabular samples. We will explain the design of the differentiable constraint layer that ensures all domain constraints are satisfied.</p> <p>Following that, in Step 2, we will demonstrate how (C-DGMs) can be extended to C-AdvDGMs to generate adversarial examples capable of bypassing machine learning models. We will explore the incorporation of adversarial loss and architectural modifications.</p> <h2 id="1-ghamizi-et-al-search-based-adversarial-testing-and-improvement-of-constrained-credit-scoring-systems-fse-2020"><small>1. <a href="https://arxiv.org/abs/1708.06131">Ghamizi et al., “Search-based adversarial testing and improvement of constrained credit scoring systems.”, FSE 2020</a></small></h2> <p><strong>Note:</strong> <em>This blog series is based on research about the realism of current adversarial attacks from my time at the SerVal group, SnT (University of Luxembourg). It’s an easy-to-digest format for anyone interested in the topic, especially those who may not have time (or willingness) to read our full papers.</em></p> <p><em>The work and results presented here are a team effort, including <a href="https://maxcordy.github.io/">Asst. Prof. Dr. Maxime Cordy</a>, <a href="https://scholar.google.com/citations?user=4RhGnOoAAAAJ&amp;hl=en&amp;oi=ao">Dr. Thibault Simonetto</a>, <a href="https://scholar.google.com/citations?user=UcvKgR0AAAAJ&amp;hl=fr">Dr. Salah Ghamizi</a>, <a href="https://scholar.google.com/citations?user=KcGsVdIAAAAJ&amp;hl=fr&amp;oi=ao">(to be Dr.) Mohamed Djilani</a>, <a href="https://mihaela-stoian.github.io/">(to be Dr.) Mihaela C. Stoian</a> and <a href="https://egiunchiglia.github.io/">Asst. Prof. Dr. Eleonora Giunchiglia</a>.</em></p> <p><em>If you want to dig deeper into the results or specific subtopics, check out the papers linked in each blog post.</em></p>]]></content><author><name></name></author><category term="research"/><category term="adversarial machine learning"/><category term="adversarial attacks"/><category term="AI/ML security"/><summary type="html"><![CDATA[In our previous posts, we highlighted how adversarial attacks designed in controlled environments often fail to reflect real-world scenarios. So far, we have focused on images and text. In this post we will move to tabular data, allegedly one of the most widely used formats in practice, powering critical systems such as financial decision-making, medical diagnoses, and automated risk assessments. Tabular AI/ML applications are equally vulnerable to adversarial attacks. Here, we will explore how to construct adversarial attacks specifically for tabular data, with a focus on maintaining realism and practical relevance.]]></summary></entry><entry><title type="html">1. The Limits of Adversarial Text Attacks</title><link href="https://salijona.github.io/blog/2025/nlp/" rel="alternate" type="text/html" title="1. The Limits of Adversarial Text Attacks"/><published>2025-09-05T00:00:00+00:00</published><updated>2025-09-05T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/nlp</id><content type="html" xml:base="https://salijona.github.io/blog/2025/nlp/"><![CDATA[<p>In the first post of this series, we introduced adversarial attacks as a security threat to AI/ML systems. We also discussed why realism matters when evaluating and defending against them. Yet defining and reproducing “realistic” conditions is already difficult for many applications. In this post, we’re zooming in on natural language processing (NLP) to see why these challenges are even harder when the medium is language.</p> <p>For image-based adversarial attacks that we have seen so far, realism is often defined by imperceptibility. Mathematically, this is captured by minimizing the distance between the original and adversarial image using L<sub>p</sub> norms. For example, the L<sub>1</sub> norm sums up all the small differences in pixel values compared to the original image. Smaller distances mean the images look almost identical to the human eye. The goal is to make changes so subtle that people don’t notice them, while still fooling the model.</p> <p>With text, however, things are much trickier. A tiny change, such as swapping “<em>movie</em>” for “<em>mov1e</em>” might deceive a classifier, but it is immediately obvious to any human reader. This disconnect matters because, in many NLP applications, humans remain part of the decision-making loop. A phishing email with awkward phrasing probably won’t get clicked, even if a sophisticated AI/ML filter fails to catch it. A fake news headline that “feels off” will raise suspicion, even if it bypasses automated detection. Offensive language that appears altered won’t convince moderators, even if the model is fooled. In these scenarios, an attack is only truly dangerous if it can deceive both algorithms and humans. This means that creating a realistic and effective adversarial example requires thinking about how humans interpret text, not just how models process it.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/series/text_spectrum-480.webp 480w,/assets/img/series/text_spectrum-800.webp 800w,/assets/img/series/text_spectrum-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/series/text_spectrum.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To understand realism for text attacks, it helps to see it as a spectrum. At one end are unrealistic attacks that are easy for humans to detect, but may still confuse automated models. In the middle are somewhat realistic attacks that sound plausible but still raise suspicion. At the far end are highly realistic attacks designed to deceive both humans and AI/ML systems.</p> <blockquote> <p>Attack realism in text-based use cases, can be viewed as a spectrum.</p> </blockquote> <p>To illustrate, consider the phishing examples in the figure above:</p> <ul> <li> <p><strong>Unrealistic (toy attack):</strong><br/> “<em>You lottery got…!! Links wkor ///</em>”<br/> These attacks are nonsensical and grammatically broken. Humans would easily dismiss them, but some detection models might still misclassify them.</p> </li> <li> <p><strong>Somewhat realistic (model fooled, humans suspicious):</strong><br/> “<em>You have win the lottery. Click here claim your prize.</em>”<br/> The message is understandable but clunky and unpolished. Many automated filters might fail to flag it, yet most users would still hesitate to trust it.</p> </li> <li> <p><strong>Highly realistic (true threat):</strong><br/> “<em>You have won the lottery. Click this link to get your money.</em>”<br/> This represents a serious security risk. The message is fluent, grammatically correct, semantically coherent, and persuasive enough to bypass both detection systems and human judgment.</p> </li> </ul> <p>Real attackers aim for highly realistic adversarial text to maximize success while avoiding detection. Consequently, most NLP robustness tests try to preserve meaning, grammar, and natural style so the attacks resemble real-world scenarios. However, these methods often rely on mathematical proxies that miss the subtle cues humans use to judge whether a message is suspicious. Our survey of the literature shows that human perception is rarely included in evaluations: 3 of 16 studies involved no human participants, and those that did often rely on just a handful of people (1–3) assessing narrow aspects like meaning preservation or label consistency. Such limited checks overlook the broader ways humans detect suspicious text.</p> <p>To address this, we conducted one of the first large-scale studies on human perception of adversarial text, surveying 378 participants across nine state-of-the-art attack methods to see how people actually respond. We focused on two key aspects of how humans perceived the adversarial texts:</p> <ul> <li> <p><strong>Validity:</strong> Does the text preserve its original intent after being manipulated?<br/> For example, if a hateful message like “I hate all XXX” is changed to “I love XXX,” the model might be fooled, but the attacker’s goal fails because the meaning that reaches the end user has been completely reversed.</p> </li> <li> <p><strong>Naturalness:</strong> Does the text appear manipulated or computer-generated to a human reader?<br/> Even if an adversarial text is valid, one that feels awkward, unnatural, or obviously altered may fail to convince people.</p> </li> </ul> <p>The results showed that what fools models often does not fool humans. We explore these findings and their implications in our ACL paper, “<em><a href="https://aclanthology.org/2023.acl-long.491.pdf">How Do Humans Perceive Adversarial Text? A Reality Check on the Validity and Naturalness of Word-Based Adversarial Attacks</a></em>”.</p> <ul> <li> <p><strong>Validity:</strong> Humans correctly understood the original intent of about 72% of adversarial texts, compared to nearly 89% for the original texts. This shows that many attacks produce a significant portion of invalid adversarial examples.</p> </li> <li> <p><strong>Naturalness:</strong> Adversarial texts also stood out as unnatural. Over 60% of participants flagged them as likely computer-generated, compared to just 21% for the original texts. When asked to pinpoint the changes, participants identified the altered words in roughly 50% of the cases. This shows that while adversarial examples may fool models, they often fail the human “readability and believability” test.</p> </li> </ul> <p>These results show that even the most advanced attacks today often wouldn’t succeed in real-world settings where humans are part of the system. This is relevant for cases like content moderation, or misinformation control, where human judgment remains a critical part of the system, as we discussed in the introduction.</p> <p>Out of curiosity, we also explored why certain texts trigger human suspicion. From there, we could observe the following trends:</p> <ul> <li><strong>Clear meaning matters:</strong> Texts that preserved their original intent and were easy to understand were generally seen as more natural, while messages with unclear or distorted meaning were much more likely to be flagged as “computer-generated.”</li> <li><strong>Grammar errors, not as much:</strong> Minor grammar mistakes didn’t necessarily make a text feel unnatural if the meaning was clear. But once a message became awkward or nonsensical, it immediately stood out to readers.</li> </ul> <p>These were the main results, and for full details you can check out our paper. Few were the main lessons, from this large-scale human study. It became clear that human psychology plays a major role in whether an attack succeeds. Focusing only on model performance misses an important point: human judgment gives a more realistic perspective for many applications. Future research on adversarial testing in NLP should focus on crafting examples that are semantically valid, natural, and persuasive, while keeping humans in the loop to accurately assess real-world threat potential.</p> <p><em>A small disclaimer</em>: Our study predates the recent surge in Large Language Models (LLMs). These systems generate fluent, convincing text that may appear far less suspicious, posing new challenges for adversarial NLP research.</p> <hr/> <p><strong>Note:</strong> <em>This blog series is based on research about the realism of current adversarial attacks from my time at the SerVal group, SnT (University of Luxembourg). It’s an easy-to-digest format for anyone interested in the topic, especially those who may not have time (or willingness) to read our full papers.</em></p> <p><em>The work and results presented here are a team effort, including <a href="https://maxcordy.github.io/">Asst. Prof. Dr. Maxime Cordy</a>, <a href="https://scholar.google.com/citations?user=4RhGnOoAAAAJ&amp;hl=en&amp;oi=ao">Dr. Thibault Simonetto</a>, <a href="https://scholar.google.com/citations?user=UcvKgR0AAAAJ&amp;hl=fr">Dr. Salah Ghamizi</a>, <a href="https://scholar.google.com/citations?user=KcGsVdIAAAAJ&amp;hl=fr&amp;oi=ao">(to be Dr.) Mohamed Djilani</a>, <a href="https://mihaela-stoian.github.io/">(to be Dr.) Mihaela C. Stoian</a> and <a href="https://egiunchiglia.github.io/">Asst. Prof. Dr. Eleonora Giunchiglia</a>.</em></p> <p><em>If you want to dig deeper into the results or specific subtopics, check out the papers linked in each blog post.</em></p>]]></content><author><name></name></author><category term="research"/><category term="adversarial machine learning"/><category term="adversarial attacks"/><category term="AI/ML security"/><summary type="html"><![CDATA[When AI Gets Fooled but Humans Don’t.]]></summary></entry><entry><title type="html">0. AI/ML Security Is Broken Without Realistic Adversarial Testing</title><link href="https://salijona.github.io/blog/2025/intro/" rel="alternate" type="text/html" title="0. AI/ML Security Is Broken Without Realistic Adversarial Testing"/><published>2025-09-04T00:00:00+00:00</published><updated>2025-09-04T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/intro</id><content type="html" xml:base="https://salijona.github.io/blog/2025/intro/"><![CDATA[<p>Imagine testing the strength of a castle’s defensive walls with cardboard weapons. It sounds absurd, but this is often how adversarial testing in AI/ML is carried out today.</p> <p>Now, instead of a castle, imagine social media platforms. They rely on AI/ML to filter harmful content such as violence, abuse, or nudity before it reaches your screen. But what happens if someone slightly alters a harmful image so the AI/ML cannot detect it, even though a human would clearly see the problem? That is the growing challenge of adversarial attacks.</p> <p>An <strong>adversarial attack</strong> is a deliberately crafted input that deceives a machine learning model without fooling a human. To us, the image appears unchanged. To the model, it’s something entirely different. This happens because machine learning models don’t “see” the way we do. Instead of high-level conceptual features (like a face or the texture of an object), they rely on fragile mathematical patterns (such as pixel correlations), which can fail when exposed to the right perturbations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/series/lab_attack-480.webp 480w,/assets/img/series/lab_attack-800.webp 800w,/assets/img/series/lab_attack-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/series/lab_attack.png" class="img-fluid rounded z-depth-1" width="50%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It is important to note that the key word here is “<em>right</em>”. Not any random manipulation causes a model to fail; while deployed models are not perfect, they are strong enough that creating a successful adversarial example usually requires a carefully crafted input, often discovered through a time-consuming search process. In most cases, it’s not something that can be done casually or instantly.</p> <p>The first clear demonstrations of adversarial attacks came in 2014 <sup>[1,2]</sup>, when researchers showed that tiny, nearly imperceptible changes to images could cause deep neural networks to misclassify them. Since then, adversarial attacks have expanded to text, malware, financial transactions, and many other types of data. In practice, these attacks can have serious consequences: they allow malware to bypass antivirus engines, financial fraudsters to evade detection, and self-driving cars to misread traffic signs. A clear and present digital security threat!</p> <p>The rise of these attacks (and others on AI/ML systems) has not gone unnoticed. Entire companies now exist solely to secure them. Take HiddenLayer <sup>[3]</sup>, for example, a company founded by people whose firsthand experience with such attacks at an antivirus provider motivated them to develop defenses. Their strategy, like many others in the field, begins with adversarial testing, known in cybersecurity circles as “red-teaming.” This approach involves attacking your own models before anyone else can, uncovering weaknesses early and assessing the risks these systems might face.</p> <p>But here’s the catch:</p> <blockquote> <p>Most adversarial testing today is disconnected from the real world.</p> </blockquote> <p>Researchers and engineers spend hours at their computers, crafting mathematically precise tweaks, small pixel changes in an image or character swaps in text. Easy to create and measure, yes, but far from the messy and creative tactics real attackers use.</p> <p>Real-world adversaries think differently from controlled lab experiments. Their priorities are:</p> <ul> <li> <p><strong>Results, not perfection.</strong> Tiny, elegant pixel tweaks or awkward emojis over an image, the choice does not matter. They care about whatever fools both the AI and any humans monitoring it.</p> </li> <li> <p><strong>Function above all.</strong> Every attack must still work in practice: transactions must process, malware must execute, sentences must make sense.</p> </li> <li> <p><strong>The full system.</strong> Attackers do not stop at the model. They probe user behavior, business logic, and human oversight, exploiting whichever link breaks first.</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/series/real_attack-480.webp 480w,/assets/img/series/real_attack-800.webp 800w,/assets/img/series/real_attack-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/series/real_attack.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>When adversarial testing ignores real-world attacks, it is like testing a castle’s walls with cardboard weapons. The walls may seem strong, but real threats go unchecked. The result is a false sense of security: AI systems that survive controlled tests may still fail against actual adversaries. That is why every defense must start with realism. Only by reflecting how attackers truly behave can we prepare models for the threats they will face in practice.</p> <p>Realistic testing recreates practical conditions. It is not about making the process harder for researchers and AI/ML Security practitioners; it is about preparing for genuine risks rather than idealized scenarios. Security tests that ignore real adversary behavior are not just incomplete, they can be misleading.</p> <p>This blog series is dedicated to closing the realism gap in adversarial testing and defense. Just as a castle cannot rely on cardboard weapons to prove its strength, AI systems cannot rely on idealized tests to ensure security. In the coming posts, we will explore how realism plays out across different domains, from financial fraud to malware detection, and why it is important not only for finding vulnerabilities but also for building resilient defenses that can withstand real-world adversaries.</p> <p><small>1. <a href="https://arxiv.org/abs/1708.06131">Biggio et al., <em>Evasion Attacks against Machine Learning at Test Time</em>, ECML 2013</a></small></p> <p><small>2. <a href="https://arxiv.org/abs/1312.6199">Szegedy et al., <em>Intriguing properties of neural networks</em>, ICLR 2014</a></small></p> <p><small>3. <a href="https://hiddenlayer.com/company/">HiddenLayer Company</a></small></p> <hr/> <p><strong>Note:</strong> <em>This blog series is based on research about the realism of current adversarial attacks from my time at the SerVal group, SnT (University of Luxembourg). It’s an easy-to-digest format for anyone interested in the topic, especially those who may not have time (or willingness) to read our full papers.</em></p> <p><em>The work and results presented here are a team effort, including <a href="https://maxcordy.github.io/">Asst. Prof. Dr. Maxime Cordy</a>, <a href="https://scholar.google.com/citations?user=4RhGnOoAAAAJ&amp;hl=en&amp;oi=ao">Dr. Thibault Simonetto</a>, <a href="https://scholar.google.com/citations?user=UcvKgR0AAAAJ&amp;hl=fr">Dr. Salah Ghamizi</a>, <a href="https://scholar.google.com/citations?user=KcGsVdIAAAAJ&amp;hl=fr&amp;oi=ao">(to be Dr.) Mohamed Djilani</a>, <a href="https://mihaela-stoian.github.io/">(to be Dr.) Mihaela C. Stoian</a> and <a href="https://egiunchiglia.github.io/">Asst. Prof. Dr. Eleonora Giunchiglia</a>.</em></p> <p><em>If you want to dig deeper into the results or specific subtopics, check out the papers linked in each blog post.</em></p>]]></content><author><name></name></author><category term="research"/><category term="adversarial machine learning"/><category term="adversarial attacks"/><category term="AI/ML security"/><summary type="html"><![CDATA[You can’t secure a castle by testing it with cardboard swords.]]></summary></entry><entry><title type="html">Tell me a story #2 - Reflections After Five Years in Academia</title><link href="https://salijona.github.io/blog/2025/tell-me-a-story-2-reflections-after-five-years-in-academia/" rel="alternate" type="text/html" title="Tell me a story #2 - Reflections After Five Years in Academia"/><published>2025-08-01T00:00:00+00:00</published><updated>2025-08-01T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/tell-me-a-story-2---reflections-after-five-years-in-academia</id><content type="html" xml:base="https://salijona.github.io/blog/2025/tell-me-a-story-2-reflections-after-five-years-in-academia/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[What I’d Tell Anyone Considering a PhD]]></summary></entry><entry><title type="html">The Forgotten Domain: Adversarial Machine Learning for Tabular Data</title><link href="https://salijona.github.io/blog/2025/slr/" rel="alternate" type="text/html" title="The Forgotten Domain: Adversarial Machine Learning for Tabular Data"/><published>2025-07-31T00:00:00+00:00</published><updated>2025-07-31T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/slr</id><content type="html" xml:base="https://salijona.github.io/blog/2025/slr/"><![CDATA[<p>Machine learning is now embedded in some of the highest-stakes decision systems: credit approvals, fraud detection, cybersecurity alerts, and even medical diagnoses. And what powers many of these systems behind the scenes? Not images or text but rows and columns. Spreadsheets. Tabular data.</p> <p>Yet when it comes to AI security, especially in the context of adversarial machine learning, tabular models have been overlooked. While the security of vision and NLP models get the headlines, the models most often deployed in industry (decision trees, gradient-boosted models, tabular neural networks) get far less attention when it comes to adversarial threats.</p> <p>That gap matters. Because if we want trustworthy AI, we can’t ignore the security risks in the systems we actually use.</p> <p>To better understand this overlooked domain, we conducted the first systematic literature review (SLR) focused entirely on adversarial attacks against tabular machine learning. Our paper, <strong>“Insights on Adversarial Attacks for Tabular Machine Learning via a Systematic Literature Review”</strong>, is now available on <a href="https://arxiv.org/abs/2506.15506">arXiv</a>.</p> <p>We analyzed <strong>53 papers</strong> published <strong>between 2018 and early 2025</strong> to answer three main questions:</p> <ul> <li>What does the research landscape look like?</li> <li>How are these attacks constructed?</li> <li>Do they actually work under real-world constraints?</li> </ul> <h2 id="1-a-niche-field-fragmented-and-domain-heavy">1. A Niche Field, Fragmented and Domain-Heavy</h2> <p>Interest in adversarial attacks on tabular ML models grew steadily from 2018 until around 2021, but has since stagnated. The field is still small and fragmented: the 53 papers we found are spread across 46 different venues, with little overlap.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/slr/rq1-480.webp 480w,/assets/img/slr/rq1-800.webp 800w,/assets/img/slr/rq1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/slr/rq1.jpg" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>About half of the research proposes general-purpose attacks, while the rest focuses on specific domains. Cybersecurity dominates the field, perhaps unsurprisingly, given the naturally adversarial environment and the higher risk of attacks. Finance comes next, with a few papers tackling credit scoring and fraud detection.</p> <p>But overall, this is still a fragmented frontier with no central community.</p> <h2 id="2-attack-design-many-techniques-few-standards">2. Attack Design: Many Techniques, Few Standards</h2> <p>We looked at 61 unique attacks proposed in the reviewed papers and found both clear patterns and notable gaps.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/slr/rq2-480.webp 480w,/assets/img/slr/rq2-800.webp 800w,/assets/img/slr/rq2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/slr/rq2.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Most attacks target classification problems, especially binary classification, even though many real-world tabular applications (like pricing or forecasting) involve regression or multi-class outputs.Interestingly, about half the attacks assume black-box access. That’s a good sign: it reflects more realistic threat scenarios. In terms of design, most attacks can be grouped into four high-level strategies:</p> <ul> <li><strong>Gradient-based</strong>: Mostly borrowed from computer vision attacks (e.g., PGD, C&amp;W) but adapted for tabular constraints.</li> <li><strong>Gradient-free</strong>: Include rule-based methods, evolutionary search, and other heuristics. These are flexible but often inconsistent.</li> <li><strong>Learning-based</strong>: Often built with GANs, especially in cybersecurity contexts. But many of these reinvent the wheel and rarely compare to prior work.</li> <li><strong>Hybrid</strong>: Combine strategies, e.g., switching from gradient optimization to search heuristics. Still rare, but promising.</li> </ul> <p>One persistent issue is low reproducibility: only about one-third of the attacks provide public code. This lack of openness makes it difficult to compare methods fairly and slows down progress in the field.</p> <h2 id="3-do-these-attacks-work-in-the-real-world">3. Do These Attacks Work in the Real World?</h2> <p>It’s one thing to demonstrate an attack in a benchmark setting. It’s another to successfully execute that attack in real-world environments where inputs are constrained, systems are monitored, and decisions often reviewed by humans. We evaluated each paper against eight practical criteria measuring not just success, but real-world viability.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/slr/rq3-480.webp 480w,/assets/img/slr/rq3-800.webp 800w,/assets/img/slr/rq3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/slr/rq3.jpg" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>Efficacy</strong>: All papers measure this, usually via attack success rate or accuracy drop. But success on clean benchmarks doesn’t guarantee usefulness in live systems.</li> <li><strong>Efficiency</strong>: Very few papers report runtime or resource costs. Yet many attacks (especially heuristic ones) could be too slow to be practical.</li> <li><strong>Transferability</strong>: Most attacks target a specific model. Few check if adversarial examples work across architectures, which is key in black-box settings.</li> <li><strong>Feasibility</strong>: Some attacks impose practical constraints (e.g., value ranges or valid encodings), but many assume idealized settings, ignoring feature accessibility, feature dependencies, or domain-specific rules.</li> <li><strong>Semantic Preservation</strong>: Does the modified input still make sense? A technically valid change (e.g., flipping “employed” to “unemployed”) may pass unnoticed by a model, but could violate the underlying meaning or ground truth of the example.</li> <li><strong>Plausibility</strong>: Would a human reviewing the input find it believable? In computer vision this is important, but only a few studies on tabular ML check for it. Some even argue plausibility may not be relevant for tabular attacks, reflecting an ongoing debate in the field.</li> <li><strong>Defense Awareness</strong>: Attacks are often tested on unprotected models. In reality, defenses like input validation, adversarial training, and monitoring are common.</li> <li><strong>Dataset Suitability</strong>: Too many studies rely on small, outdated academic datasets (like Iris or Wine) that don’t reflect real-world complexity or risk.</li> </ul> <p>Most papers addressed efficacy, and some considered feasibility. But beyond those, few went further. Most studies covered just two to six of the eight practical criteria, with four being the most common.</p> <h2 id="what-we-learned">What We Learned</h2> <p>Adversarial attacks on tabular models aren’t just academic exercises. In fields like finance, healthcare, and cybersecurity, these vulnerabilities can have real-world consequences. And yet, the field remains immature.</p> <p>We found innovative techniques tested only in idealized settings, with limited practical relevance. Many papers measure success as a drop in model accuracy, but rarely ask deeper questions: Is the attack realistic? Is the input plausible? Would it even be accepted by the system?</p> <p>We also found a lack of shared standards. Key concepts like semantic preservation and plausibility are used inconsistently and often left undefined. Code is frequently missing. Benchmarks are almost non-existent. And most attacks are tested on just one dataset and undefended models, making comparisons and cumulative progress difficult.</p> <h2 id="where-the-field-goes-next">Where the Field Goes Next</h2> <p>Despite these issues, the space for meaningful research in this domain remains open, especially as paradigms like pretraining, multitask learning, and tabular foundation models continue to emerge. What’s needed now isn’t just more attacks, but a stronger research foundation:</p> <ul> <li>Clear, shared definitions of robustness in tabular ML</li> <li>Benchmarks that reflect real-world constraints</li> <li>Broader, more realistic datasets</li> <li>Reproducible code and cross-domain evaluations</li> </ul> <p>If we want machine learning to be secure and trustworthy in critical systems, from credit scoring to malware detection, we need to stop treating tabular data as an afterthought.</p> <p>It’s time to give it the attention, clarity, and care it deserves.</p>]]></content><author><name></name></author><category term="research"/><category term="adversarial machine learning"/><category term="tabular data"/><category term="AI/ML security"/><summary type="html"><![CDATA[What happens when the most common data format in ML is also the least protected?]]></summary></entry></feed>