<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://salijona.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://salijona.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-04T14:30:57+00:00</updated><id>https://salijona.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">00. AI/ML Security Is Broken Without Realistic Adversarial Testing</title><link href="https://salijona.github.io/blog/2025/intro/" rel="alternate" type="text/html" title="00. AI/ML Security Is Broken Without Realistic Adversarial Testing"/><published>2025-09-04T00:00:00+00:00</published><updated>2025-09-04T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/intro</id><content type="html" xml:base="https://salijona.github.io/blog/2025/intro/"><![CDATA[<p>Imagine testing the strength of a castle’s defensive walls with cardboard weapons. It sounds absurd, but this is often how adversarial testing in AI/ML is carried out today.</p> <p>Now, instead of a castle, imagine social media platforms. They rely on AI/ML to filter harmful content such as violence, abuse, or nudity before it reaches your screen. But what happens if someone slightly alters a harmful image so the AI/ML cannot detect it, even though a human would clearly see the problem? That is the growing challenge of adversarial attacks.</p> <p>An <strong>adversarial attack</strong> is a deliberately crafted input that deceives a machine learning model without fooling a human. To us, the image appears unchanged. To the model, it’s something entirely different. This happens because machine learning models don’t “see” the way we do. Instead of high-level conceptual features (like a face or the texture of an object), they rely on fragile mathematical patterns (such as pixel correlations), which can fail when exposed to the right perturbations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/series/lab_attack-480.webp 480w,/assets/img/series/lab_attack-800.webp 800w,/assets/img/series/lab_attack-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/series/lab_attack.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It is important to note that the key word here is “<em>right</em>”. Not any random manipulation causes a model to fail; while deployed models are not perfect, they are strong enough that creating a successful adversarial example usually requires a carefully crafted input, often discovered through a time-consuming search process. In most cases, it’s not something that can be done casually or instantly.</p> <p>The first clear demonstrations of adversarial attacks came in 2014, when researchers showed that tiny, nearly imperceptible changes to images could cause deep neural networks to misclassify them. Since then, adversarial attacks have expanded to text, malware, financial transactions, and many other types of data. In practice, these attacks can have serious consequences: they allow malware to bypass defenses, fraudsters to evade detection, and self-driving cars to misread traffic signs. A clear and present digital security threat!</p> <p>The rise of these attacks (and others on AI/ML systems) has not gone unnoticed. Entire companies now exist solely to secure them. Take HiddenLayer, for example, a company founded by people whose firsthand experience with such attacks at an antivirus provider motivated them to develop defenses. Their strategy, like many others in the field, begins with adversarial testing, known in cybersecurity circles as “red-teaming.” This approach involves attacking your own models before anyone else can, uncovering weaknesses early and assessing the risks these systems might face.</p> <p>But here’s the catch:</p> <blockquote> <p>“Most adversarial testing today is disconnected from the real world.”</p> </blockquote> <p>Researchers and engineers spend hours at their computers, crafting mathematically precise tweaks, small pixel changes in an image or character swaps in text. Easy to create and measure, yes, but far from the messy and creative tactics real attackers use.</p> <p>Real-world adversaries think differently from controlled lab experiments. Their priorities are:</p> <ul> <li> <p><strong>Results, not perfection.</strong> Tiny, elegant pixel tweaks or awkward emojis over an image, the choice does not matter. They care about whatever fools both the AI and any humans monitoring it.</p> </li> <li> <p><strong>Function above all.</strong> Every attack must still work in practice: transactions must process, malware must execute, sentences must make sense.</p> </li> <li> <p><strong>The full system.</strong> Attackers do not stop at the model. They probe user behavior, business logic, and human oversight, exploiting whichever link breaks first.</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/series/real_attack-480.webp 480w,/assets/img/series/real_attack-800.webp 800w,/assets/img/series/real_attack-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/series/real_attack.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>When adversarial testing ignores real-world attacks, it is like testing a castle’s walls with cardboard weapons. The walls may seem strong, but real threats go unchecked. The result is a false sense of security: AI systems that survive controlled tests may still fail against actual adversaries. That is why every defense must start with realism. Only by reflecting how attackers truly behave can we prepare models for the threats they will face in practice.</p> <p>Realistic testing recreates practical conditions. It is not about making the process harder for researchers and AI/ML Security practitioners; it is about preparing for genuine risks rather than idealized scenarios. Security tests that ignore real adversary behavior are not just incomplete, they can be misleading.</p> <p>This blog series is dedicated to closing the realism gap in adversarial testing and defense. Just as a castle cannot rely on cardboard weapons to prove its strength, AI systems cannot rely on idealized tests to ensure security. In the coming posts, we will explore how realism plays out across different domains, from financial fraud to malware detection, and why it is important not only for finding vulnerabilities but also for building resilient defenses that can withstand real-world adversaries.</p> <hr/> <p><strong>Note:</strong> <em>This blog series is based on research about the realism of current adversarial attacks from my time at the SerVal group, SnT (University of Luxembourg). It’s an easy-to-digest format for anyone interested in the topic, especially those who may not have time (or willingness) to read our full papers. The work and results presented here are a team effort, including <a href="https://maxcordy.github.io/">Asst. Prof. Dr. Maxime Cordy</a>, <a href="https://scholar.google.com/citations?user=4RhGnOoAAAAJ&amp;hl=en&amp;oi=ao">Dr. Thibault Simonetto</a>, <a href="https://scholar.google.com/citations?user=UcvKgR0AAAAJ&amp;hl=fr">Dr. Salah Ghamizi</a>, <a href="https://scholar.google.com/citations?user=KcGsVdIAAAAJ&amp;hl=fr&amp;oi=ao">(to be Dr.) Mohamed Djilani</a>, <a href="https://mihaela-stoian.github.io/">(to be Dr.) Mihaela C. Stoian</a> and <a href="https://egiunchiglia.github.io/">Asst. Prof. Dr. Eleonora Giunchiglia</a>. If you want to dig deeper into the results or specific subtopics, check out the papers linked in each blog post.</em></p>]]></content><author><name></name></author><category term="research"/><category term="adversarial machine learning"/><category term="adversarial attacks"/><category term="AI/ML security"/><summary type="html"><![CDATA[You can’t secure a castle by testing it with cardboard swords.]]></summary></entry><entry><title type="html">Tell me a story #2 - Reflections After Five Years in Academia</title><link href="https://salijona.github.io/blog/2025/tell-me-a-story-2-reflections-after-five-years-in-academia/" rel="alternate" type="text/html" title="Tell me a story #2 - Reflections After Five Years in Academia"/><published>2025-08-01T00:00:00+00:00</published><updated>2025-08-01T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/tell-me-a-story-2---reflections-after-five-years-in-academia</id><content type="html" xml:base="https://salijona.github.io/blog/2025/tell-me-a-story-2-reflections-after-five-years-in-academia/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[What I’d Tell Anyone Considering a PhD]]></summary></entry><entry><title type="html">The Forgotten Domain: Adversarial Machine Learning for Tabular Data</title><link href="https://salijona.github.io/blog/2025/slr/" rel="alternate" type="text/html" title="The Forgotten Domain: Adversarial Machine Learning for Tabular Data"/><published>2025-07-31T00:00:00+00:00</published><updated>2025-07-31T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/slr</id><content type="html" xml:base="https://salijona.github.io/blog/2025/slr/"><![CDATA[<p>Machine learning is now embedded in some of the highest-stakes decision systems: credit approvals, fraud detection, cybersecurity alerts, and even medical diagnoses. And what powers many of these systems behind the scenes? Not images or text but rows and columns. Spreadsheets. Tabular data.</p> <p>Yet when it comes to AI security, especially in the context of adversarial machine learning, tabular models have been overlooked. While the security of vision and NLP models get the headlines, the models most often deployed in industry (decision trees, gradient-boosted models, tabular neural networks) get far less attention when it comes to adversarial threats.</p> <p>That gap matters. Because if we want trustworthy AI, we can’t ignore the security risks in the systems we actually use.</p> <p>To better understand this overlooked domain, we conducted the first systematic literature review (SLR) focused entirely on adversarial attacks against tabular machine learning. Our paper, <strong>“Insights on Adversarial Attacks for Tabular Machine Learning via a Systematic Literature Review”</strong>, is now available on <a href="https://arxiv.org/abs/2506.15506">arXiv</a>.</p> <p>We analyzed <strong>53 papers</strong> published <strong>between 2018 and early 2025</strong> to answer three main questions:</p> <ul> <li>What does the research landscape look like?</li> <li>How are these attacks constructed?</li> <li>Do they actually work under real-world constraints?</li> </ul> <h2 id="1-a-niche-field-fragmented-and-domain-heavy">1. A Niche Field, Fragmented and Domain-Heavy</h2> <p>Interest in adversarial attacks on tabular ML models grew steadily from 2018 until around 2021, but has since stagnated. The field is still small and fragmented: the 53 papers we found are spread across 46 different venues, with little overlap.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/slr/rq1-480.webp 480w,/assets/img/slr/rq1-800.webp 800w,/assets/img/slr/rq1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/slr/rq1.jpg" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>About half of the research proposes general-purpose attacks, while the rest focuses on specific domains. Cybersecurity dominates the field, perhaps unsurprisingly, given the naturally adversarial environment and the higher risk of attacks. Finance comes next, with a few papers tackling credit scoring and fraud detection.</p> <p>But overall, this is still a fragmented frontier with no central community.</p> <h2 id="2-attack-design-many-techniques-few-standards">2. Attack Design: Many Techniques, Few Standards</h2> <p>We looked at 61 unique attacks proposed in the reviewed papers and found both clear patterns and notable gaps.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/slr/rq2-480.webp 480w,/assets/img/slr/rq2-800.webp 800w,/assets/img/slr/rq2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/slr/rq2.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Most attacks target classification problems, especially binary classification, even though many real-world tabular applications (like pricing or forecasting) involve regression or multi-class outputs.Interestingly, about half the attacks assume black-box access. That’s a good sign: it reflects more realistic threat scenarios. In terms of design, most attacks can be grouped into four high-level strategies:</p> <ul> <li><strong>Gradient-based</strong>: Mostly borrowed from computer vision attacks (e.g., PGD, C&amp;W) but adapted for tabular constraints.</li> <li><strong>Gradient-free</strong>: Include rule-based methods, evolutionary search, and other heuristics. These are flexible but often inconsistent.</li> <li><strong>Learning-based</strong>: Often built with GANs, especially in cybersecurity contexts. But many of these reinvent the wheel and rarely compare to prior work.</li> <li><strong>Hybrid</strong>: Combine strategies, e.g., switching from gradient optimization to search heuristics. Still rare, but promising.</li> </ul> <p>One persistent issue is low reproducibility: only about one-third of the attacks provide public code. This lack of openness makes it difficult to compare methods fairly and slows down progress in the field.</p> <h2 id="3-do-these-attacks-work-in-the-real-world">3. Do These Attacks Work in the Real World?</h2> <p>It’s one thing to demonstrate an attack in a benchmark setting. It’s another to successfully execute that attack in real-world environments where inputs are constrained, systems are monitored, and decisions often reviewed by humans. We evaluated each paper against eight practical criteria measuring not just success, but real-world viability.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/slr/rq3-480.webp 480w,/assets/img/slr/rq3-800.webp 800w,/assets/img/slr/rq3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/slr/rq3.jpg" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>Efficacy</strong>: All papers measure this, usually via attack success rate or accuracy drop. But success on clean benchmarks doesn’t guarantee usefulness in live systems.</li> <li><strong>Efficiency</strong>: Very few papers report runtime or resource costs. Yet many attacks (especially heuristic ones) could be too slow to be practical.</li> <li><strong>Transferability</strong>: Most attacks target a specific model. Few check if adversarial examples work across architectures, which is key in black-box settings.</li> <li><strong>Feasibility</strong>: Some attacks impose practical constraints (e.g., value ranges or valid encodings), but many assume idealized settings, ignoring feature accessibility, feature dependencies, or domain-specific rules.</li> <li><strong>Semantic Preservation</strong>: Does the modified input still make sense? A technically valid change (e.g., flipping “employed” to “unemployed”) may pass unnoticed by a model, but could violate the underlying meaning or ground truth of the example.</li> <li><strong>Plausibility</strong>: Would a human reviewing the input find it believable? In computer vision this is important, but only a few studies on tabular ML check for it. Some even argue plausibility may not be relevant for tabular attacks, reflecting an ongoing debate in the field.</li> <li><strong>Defense Awareness</strong>: Attacks are often tested on unprotected models. In reality, defenses like input validation, adversarial training, and monitoring are common.</li> <li><strong>Dataset Suitability</strong>: Too many studies rely on small, outdated academic datasets (like Iris or Wine) that don’t reflect real-world complexity or risk.</li> </ul> <p>Most papers addressed efficacy, and some considered feasibility. But beyond those, few went further. Most studies covered just two to six of the eight practical criteria, with four being the most common.</p> <h2 id="what-we-learned">What We Learned</h2> <p>Adversarial attacks on tabular models aren’t just academic exercises. In fields like finance, healthcare, and cybersecurity, these vulnerabilities can have real-world consequences. And yet, the field remains immature.</p> <p>We found innovative techniques tested only in idealized settings, with limited practical relevance. Many papers measure success as a drop in model accuracy, but rarely ask deeper questions: Is the attack realistic? Is the input plausible? Would it even be accepted by the system?</p> <p>We also found a lack of shared standards. Key concepts like semantic preservation and plausibility are used inconsistently and often left undefined. Code is frequently missing. Benchmarks are almost non-existent. And most attacks are tested on just one dataset and undefended models, making comparisons and cumulative progress difficult.</p> <h2 id="where-the-field-goes-next">Where the Field Goes Next</h2> <p>Despite these issues, the space for meaningful research in this domain remains open, especially as paradigms like pretraining, multitask learning, and tabular foundation models continue to emerge. What’s needed now isn’t just more attacks, but a stronger research foundation:</p> <ul> <li>Clear, shared definitions of robustness in tabular ML</li> <li>Benchmarks that reflect real-world constraints</li> <li>Broader, more realistic datasets</li> <li>Reproducible code and cross-domain evaluations</li> </ul> <p>If we want machine learning to be secure and trustworthy in critical systems, from credit scoring to malware detection, we need to stop treating tabular data as an afterthought.</p> <p>It’s time to give it the attention, clarity, and care it deserves.</p>]]></content><author><name></name></author><category term="research"/><category term="adversarial machine learning"/><category term="tabular data"/><category term="AI/ML security"/><summary type="html"><![CDATA[What happens when the most common data format in ML is also the least protected?]]></summary></entry></feed>