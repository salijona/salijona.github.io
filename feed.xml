<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://salijona.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://salijona.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-07T13:45:50+00:00</updated><id>https://salijona.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">1. The Limits of Adversarial Text Attacks</title><link href="https://salijona.github.io/blog/2025/nlp/" rel="alternate" type="text/html" title="1. The Limits of Adversarial Text Attacks"/><published>2025-09-05T00:00:00+00:00</published><updated>2025-09-05T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/nlp</id><content type="html" xml:base="https://salijona.github.io/blog/2025/nlp/"><![CDATA[<p>In the first post of this series, we introduced adversarial attacks as a security threat to AI/ML systems. We also discussed why realism matters when evaluating and defending against them. Yet defining and reproducing “realistic” conditions is already difficult for many applications. In this post, we’re zooming in on natural language processing (NLP) to see why these challenges are even harder when the medium is language.</p> <p>For image-based adversarial attacks that we have seen so far, realism is often defined by imperceptibility. Mathematically, this is captured by minimizing the distance between the original and adversarial image using L<sub>p</sub> norms. For example, the L<sub>1</sub> norm sums up all the small differences in pixel values compared to the original image. Smaller distances mean the images look almost identical to the human eye. The goal is to make changes so subtle that people don’t notice them, while still fooling the model.</p> <p>With text, however, things are much trickier. A tiny change, such as swapping “<em>movie</em>” for “<em>mov1e</em>” might deceive a classifier, but it is immediately obvious to any human reader. This disconnect matters because, in many NLP applications, humans remain part of the decision-making loop. A phishing email with awkward phrasing probably won’t get clicked, even if a sophisticated AI/ML filter fails to catch it. A fake news headline that “feels off” will raise suspicion, even if it bypasses automated detection. Offensive language that appears altered won’t convince moderators, even if the model is fooled. In these scenarios, an attack is only truly dangerous if it can deceive both algorithms and humans. This means that creating a realistic and effective adversarial example requires thinking about how humans interpret text, not just how models process it.</p> <blockquote> <p>Attack realism in text-based use cases, can be viewed as a spectrum.</p> </blockquote> <p>At one end are unrealistic attacks that are easy for humans to detect, but may still confuse automated models. In the middle are somewhat realistic attacks that sound plausible but still raise suspicion. At the far end are highly realistic attacks designed to deceive both humans and AI/ML systems.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/series/text_spectrum-480.webp 480w,/assets/img/series/text_spectrum-800.webp 800w,/assets/img/series/text_spectrum-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/series/text_spectrum.png" class="img-fluid rounded z-depth-1" width="80%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To illustrate, consider the phishing examples in the figure:</p> <ul> <li> <p><strong>Unrealistic (toy attack):</strong><br/> “<em>You lottery got…!! Links wkor ///</em>”<br/> These attacks are nonsensical and grammatically broken. Humans would easily dismiss them, but some detection models might still misclassify them.</p> </li> <li> <p><strong>Somewhat realistic (model fooled, humans suspicious):</strong><br/> “<em>You have win the lottery. Click here claim your prize.</em>”<br/> The message is understandable but clunky and unpolished. Many automated filters might fail to flag it, yet most users would still hesitate to trust it.</p> </li> <li> <p><strong>Highly realistic (true threat):</strong><br/> “<em>You have won the lottery. Click this link to get your money.</em>”<br/> This represents a serious security risk. The message is fluent, grammatically correct, semantically coherent, and persuasive enough to bypass both detection systems and human judgment.</p> </li> </ul> <p>Real attackers aim for highly realistic adversarial text to maximize success while avoiding detection. Consequently, most NLP robustness tests try to preserve meaning, grammar, and natural style so the attacks resemble real-world scenarios. However, these methods often rely on mathematical proxies that miss the subtle cues humans use to judge whether a message is suspicious. Our survey of the literature shows that human perception is rarely included in evaluations: 3 of 16 studies involved no human participants, and those that did often rely on just a handful of people (1–3) assessing narrow aspects like meaning preservation or label consistency. Such limited checks overlook the broader ways humans detect suspicious text.</p> <p>To address this, we conducted one of the first large-scale studies on human perception of adversarial text, surveying 378 participants across nine state-of-the-art attack methods to see how people actually respond. We focused on two key aspects of how humans perceived the adversarial texts:</p> <ul> <li> <p><strong>Validity:</strong> Does the text preserve its original intent after being manipulated?<br/> For example, if a hateful message like “I hate all XXX” is changed to “I love XXX,” the model might be fooled, but the attacker’s goal fails because the meaning that reaches the end user has been completely reversed.</p> </li> <li> <p><strong>Naturalness:</strong> Does the text appear manipulated or computer-generated to a human reader?<br/> Even if an adversarial text is valid, one that feels awkward, unnatural, or obviously altered may fail to convince people.</p> </li> </ul> <p>The results showed that what fools models often does not fool humans. We explore these findings and their implications in our ACL paper, “<a href="https://aclanthology.org/2023.acl-long.491.pdf"><em>How Do Humans Perceive Adversarial Text? A Reality Check on the Validity and Naturalness of Word-Based Adversarial Attacks</em></a>”.</p> <ul> <li> <p><strong>Validity:</strong> Humans correctly understood the original intent of about 72% of adversarial texts, compared to nearly 89% for the original texts. This shows that many attacks produce a significant portion of invalid adversarial examples.</p> </li> <li> <p><strong>Naturalness:</strong> Adversarial texts also stood out as unnatural. Over 60% of participants flagged them as likely computer-generated, compared to just 21% for the original texts. When asked to pinpoint the changes, participants identified the altered words in roughly 50% of the cases. This shows that while adversarial examples may fool models, they often fail the human “readability and believability” test.</p> </li> </ul> <p>These results show that even the most advanced attacks today often wouldn’t succeed in real-world settings where humans are part of the system. This is relevant for cases like content moderation, or misinformation control, where human judgment remains a critical part of the system, as we discussed in the introduction.</p> <p>Out of curiosity, we also explored why certain texts trigger human suspicion. From there, we could observe the following trends:</p> <ul> <li><strong>Clear meaning matters:</strong> Texts that preserved their original intent and were easy to understand were generally seen as more natural, while messages with unclear or distorted meaning were much more likely to be flagged as “computer-generated.”</li> <li><strong>Grammar errors, not as much:</strong> Minor grammar mistakes didn’t necessarily make a text feel unnatural if the meaning was clear. But once a message became awkward or nonsensical, it immediately stood out to readers.</li> </ul> <p>These were the main results, and for full details you can check out our paper. Few were the main lessons, from this large-scale human study. It became clear that human psychology plays a major role in whether an attack succeeds. Focusing only on model performance misses an important point: human judgment gives a more realistic perspective for many applications. Future research on adversarial testing in NLP should focus on crafting examples that are semantically valid, natural, and persuasive, while keeping humans in the loop to accurately assess real-world threat potential.</p> <p><em>A small disclaimer</em>: Our study predates the recent surge in Large Language Models (LLMs). These systems generate fluent, convincing text that may appear far less suspicious, posing new challenges for adversarial NLP research.</p> <hr/> <p><strong>Note:</strong> <em>This blog series is based on research about the realism of current adversarial attacks from my time at the SerVal group, SnT (University of Luxembourg). It’s an easy-to-digest format for anyone interested in the topic, especially those who may not have time (or willingness) to read our full papers.</em></p> <p><em>The work and results presented here are a team effort, including <a href="https://maxcordy.github.io/">Asst. Prof. Dr. Maxime Cordy</a>, <a href="https://scholar.google.com/citations?user=4RhGnOoAAAAJ&amp;hl=en&amp;oi=ao">Dr. Thibault Simonetto</a>, <a href="https://scholar.google.com/citations?user=UcvKgR0AAAAJ&amp;hl=fr">Dr. Salah Ghamizi</a>, <a href="https://scholar.google.com/citations?user=KcGsVdIAAAAJ&amp;hl=fr&amp;oi=ao">(to be Dr.) Mohamed Djilani</a>, <a href="https://mihaela-stoian.github.io/">(to be Dr.) Mihaela C. Stoian</a> and <a href="https://egiunchiglia.github.io/">Asst. Prof. Dr. Eleonora Giunchiglia</a>.</em></p> <p><em>If you want to dig deeper into the results or specific subtopics, check out the papers linked in each blog post.</em></p>]]></content><author><name></name></author><category term="research"/><category term="adversarial machine learning"/><category term="adversarial attacks"/><category term="AI/ML security"/><summary type="html"><![CDATA[When AI Gets Fooled but Humans Don’t.]]></summary></entry><entry><title type="html">0. AI/ML Security Is Broken Without Realistic Adversarial Testing</title><link href="https://salijona.github.io/blog/2025/intro/" rel="alternate" type="text/html" title="0. AI/ML Security Is Broken Without Realistic Adversarial Testing"/><published>2025-09-04T00:00:00+00:00</published><updated>2025-09-04T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/intro</id><content type="html" xml:base="https://salijona.github.io/blog/2025/intro/"><![CDATA[<p>Imagine testing the strength of a castle’s defensive walls with cardboard weapons. It sounds absurd, but this is often how adversarial testing in AI/ML is carried out today.</p> <p>Now, instead of a castle, imagine social media platforms. They rely on AI/ML to filter harmful content such as violence, abuse, or nudity before it reaches your screen. But what happens if someone slightly alters a harmful image so the AI/ML cannot detect it, even though a human would clearly see the problem? That is the growing challenge of adversarial attacks.</p> <p>An <strong>adversarial attack</strong> is a deliberately crafted input that deceives a machine learning model without fooling a human. To us, the image appears unchanged. To the model, it’s something entirely different. This happens because machine learning models don’t “see” the way we do. Instead of high-level conceptual features (like a face or the texture of an object), they rely on fragile mathematical patterns (such as pixel correlations), which can fail when exposed to the right perturbations.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/series/lab_attack-480.webp 480w,/assets/img/series/lab_attack-800.webp 800w,/assets/img/series/lab_attack-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/series/lab_attack.png" class="img-fluid rounded z-depth-1" width="50%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It is important to note that the key word here is “<em>right</em>”. Not any random manipulation causes a model to fail; while deployed models are not perfect, they are strong enough that creating a successful adversarial example usually requires a carefully crafted input, often discovered through a time-consuming search process. In most cases, it’s not something that can be done casually or instantly.</p> <p>The first clear demonstrations of adversarial attacks came in 2014 <sup>[1,2]</sup>, when researchers showed that tiny, nearly imperceptible changes to images could cause deep neural networks to misclassify them. Since then, adversarial attacks have expanded to text, malware, financial transactions, and many other types of data. In practice, these attacks can have serious consequences: they allow malware to bypass antivirus engines, financial fraudsters to evade detection, and self-driving cars to misread traffic signs. A clear and present digital security threat!</p> <p>The rise of these attacks (and others on AI/ML systems) has not gone unnoticed. Entire companies now exist solely to secure them. Take HiddenLayer <sup>[3]</sup>, for example, a company founded by people whose firsthand experience with such attacks at an antivirus provider motivated them to develop defenses. Their strategy, like many others in the field, begins with adversarial testing, known in cybersecurity circles as “red-teaming.” This approach involves attacking your own models before anyone else can, uncovering weaknesses early and assessing the risks these systems might face.</p> <p>But here’s the catch:</p> <blockquote> <p>Most adversarial testing today is disconnected from the real world.</p> </blockquote> <p>Researchers and engineers spend hours at their computers, crafting mathematically precise tweaks, small pixel changes in an image or character swaps in text. Easy to create and measure, yes, but far from the messy and creative tactics real attackers use.</p> <p>Real-world adversaries think differently from controlled lab experiments. Their priorities are:</p> <ul> <li> <p><strong>Results, not perfection.</strong> Tiny, elegant pixel tweaks or awkward emojis over an image, the choice does not matter. They care about whatever fools both the AI and any humans monitoring it.</p> </li> <li> <p><strong>Function above all.</strong> Every attack must still work in practice: transactions must process, malware must execute, sentences must make sense.</p> </li> <li> <p><strong>The full system.</strong> Attackers do not stop at the model. They probe user behavior, business logic, and human oversight, exploiting whichever link breaks first.</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/series/real_attack-480.webp 480w,/assets/img/series/real_attack-800.webp 800w,/assets/img/series/real_attack-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/series/real_attack.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>When adversarial testing ignores real-world attacks, it is like testing a castle’s walls with cardboard weapons. The walls may seem strong, but real threats go unchecked. The result is a false sense of security: AI systems that survive controlled tests may still fail against actual adversaries. That is why every defense must start with realism. Only by reflecting how attackers truly behave can we prepare models for the threats they will face in practice.</p> <p>Realistic testing recreates practical conditions. It is not about making the process harder for researchers and AI/ML Security practitioners; it is about preparing for genuine risks rather than idealized scenarios. Security tests that ignore real adversary behavior are not just incomplete, they can be misleading.</p> <p>This blog series is dedicated to closing the realism gap in adversarial testing and defense. Just as a castle cannot rely on cardboard weapons to prove its strength, AI systems cannot rely on idealized tests to ensure security. In the coming posts, we will explore how realism plays out across different domains, from financial fraud to malware detection, and why it is important not only for finding vulnerabilities but also for building resilient defenses that can withstand real-world adversaries.</p> <p><small>1. <a href="https://arxiv.org/abs/1708.06131">Biggio et al., <em>Evasion Attacks against Machine Learning at Test Time</em>, ECML 2013</a></small></p> <p><small>2. <a href="https://arxiv.org/abs/1312.6199">Szegedy et al., <em>Intriguing properties of neural networks</em>, ICLR 2014</a></small></p> <p><small>3. <a href="https://hiddenlayer.com/company/">HiddenLayer Company</a></small></p> <hr/> <p><strong>Note:</strong> <em>This blog series is based on research about the realism of current adversarial attacks from my time at the SerVal group, SnT (University of Luxembourg). It’s an easy-to-digest format for anyone interested in the topic, especially those who may not have time (or willingness) to read our full papers.</em></p> <p><em>The work and results presented here are a team effort, including <a href="https://maxcordy.github.io/">Asst. Prof. Dr. Maxime Cordy</a>, <a href="https://scholar.google.com/citations?user=4RhGnOoAAAAJ&amp;hl=en&amp;oi=ao">Dr. Thibault Simonetto</a>, <a href="https://scholar.google.com/citations?user=UcvKgR0AAAAJ&amp;hl=fr">Dr. Salah Ghamizi</a>, <a href="https://scholar.google.com/citations?user=KcGsVdIAAAAJ&amp;hl=fr&amp;oi=ao">(to be Dr.) Mohamed Djilani</a>, <a href="https://mihaela-stoian.github.io/">(to be Dr.) Mihaela C. Stoian</a> and <a href="https://egiunchiglia.github.io/">Asst. Prof. Dr. Eleonora Giunchiglia</a>.</em></p> <p><em>If you want to dig deeper into the results or specific subtopics, check out the papers linked in each blog post.</em></p>]]></content><author><name></name></author><category term="research"/><category term="adversarial machine learning"/><category term="adversarial attacks"/><category term="AI/ML security"/><summary type="html"><![CDATA[You can’t secure a castle by testing it with cardboard swords.]]></summary></entry><entry><title type="html">Tell me a story #2 - Reflections After Five Years in Academia</title><link href="https://salijona.github.io/blog/2025/tell-me-a-story-2-reflections-after-five-years-in-academia/" rel="alternate" type="text/html" title="Tell me a story #2 - Reflections After Five Years in Academia"/><published>2025-08-01T00:00:00+00:00</published><updated>2025-08-01T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/tell-me-a-story-2---reflections-after-five-years-in-academia</id><content type="html" xml:base="https://salijona.github.io/blog/2025/tell-me-a-story-2-reflections-after-five-years-in-academia/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[What I’d Tell Anyone Considering a PhD]]></summary></entry><entry><title type="html">The Forgotten Domain: Adversarial Machine Learning for Tabular Data</title><link href="https://salijona.github.io/blog/2025/slr/" rel="alternate" type="text/html" title="The Forgotten Domain: Adversarial Machine Learning for Tabular Data"/><published>2025-07-31T00:00:00+00:00</published><updated>2025-07-31T00:00:00+00:00</updated><id>https://salijona.github.io/blog/2025/slr</id><content type="html" xml:base="https://salijona.github.io/blog/2025/slr/"><![CDATA[<p>Machine learning is now embedded in some of the highest-stakes decision systems: credit approvals, fraud detection, cybersecurity alerts, and even medical diagnoses. And what powers many of these systems behind the scenes? Not images or text but rows and columns. Spreadsheets. Tabular data.</p> <p>Yet when it comes to AI security, especially in the context of adversarial machine learning, tabular models have been overlooked. While the security of vision and NLP models get the headlines, the models most often deployed in industry (decision trees, gradient-boosted models, tabular neural networks) get far less attention when it comes to adversarial threats.</p> <p>That gap matters. Because if we want trustworthy AI, we can’t ignore the security risks in the systems we actually use.</p> <p>To better understand this overlooked domain, we conducted the first systematic literature review (SLR) focused entirely on adversarial attacks against tabular machine learning. Our paper, <strong>“Insights on Adversarial Attacks for Tabular Machine Learning via a Systematic Literature Review”</strong>, is now available on <a href="https://arxiv.org/abs/2506.15506">arXiv</a>.</p> <p>We analyzed <strong>53 papers</strong> published <strong>between 2018 and early 2025</strong> to answer three main questions:</p> <ul> <li>What does the research landscape look like?</li> <li>How are these attacks constructed?</li> <li>Do they actually work under real-world constraints?</li> </ul> <h2 id="1-a-niche-field-fragmented-and-domain-heavy">1. A Niche Field, Fragmented and Domain-Heavy</h2> <p>Interest in adversarial attacks on tabular ML models grew steadily from 2018 until around 2021, but has since stagnated. The field is still small and fragmented: the 53 papers we found are spread across 46 different venues, with little overlap.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/slr/rq1-480.webp 480w,/assets/img/slr/rq1-800.webp 800w,/assets/img/slr/rq1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/slr/rq1.jpg" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>About half of the research proposes general-purpose attacks, while the rest focuses on specific domains. Cybersecurity dominates the field, perhaps unsurprisingly, given the naturally adversarial environment and the higher risk of attacks. Finance comes next, with a few papers tackling credit scoring and fraud detection.</p> <p>But overall, this is still a fragmented frontier with no central community.</p> <h2 id="2-attack-design-many-techniques-few-standards">2. Attack Design: Many Techniques, Few Standards</h2> <p>We looked at 61 unique attacks proposed in the reviewed papers and found both clear patterns and notable gaps.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/slr/rq2-480.webp 480w,/assets/img/slr/rq2-800.webp 800w,/assets/img/slr/rq2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/slr/rq2.png" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Most attacks target classification problems, especially binary classification, even though many real-world tabular applications (like pricing or forecasting) involve regression or multi-class outputs.Interestingly, about half the attacks assume black-box access. That’s a good sign: it reflects more realistic threat scenarios. In terms of design, most attacks can be grouped into four high-level strategies:</p> <ul> <li><strong>Gradient-based</strong>: Mostly borrowed from computer vision attacks (e.g., PGD, C&amp;W) but adapted for tabular constraints.</li> <li><strong>Gradient-free</strong>: Include rule-based methods, evolutionary search, and other heuristics. These are flexible but often inconsistent.</li> <li><strong>Learning-based</strong>: Often built with GANs, especially in cybersecurity contexts. But many of these reinvent the wheel and rarely compare to prior work.</li> <li><strong>Hybrid</strong>: Combine strategies, e.g., switching from gradient optimization to search heuristics. Still rare, but promising.</li> </ul> <p>One persistent issue is low reproducibility: only about one-third of the attacks provide public code. This lack of openness makes it difficult to compare methods fairly and slows down progress in the field.</p> <h2 id="3-do-these-attacks-work-in-the-real-world">3. Do These Attacks Work in the Real World?</h2> <p>It’s one thing to demonstrate an attack in a benchmark setting. It’s another to successfully execute that attack in real-world environments where inputs are constrained, systems are monitored, and decisions often reviewed by humans. We evaluated each paper against eight practical criteria measuring not just success, but real-world viability.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/slr/rq3-480.webp 480w,/assets/img/slr/rq3-800.webp 800w,/assets/img/slr/rq3-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/slr/rq3.jpg" class="img-fluid rounded z-depth-1" width="70%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li><strong>Efficacy</strong>: All papers measure this, usually via attack success rate or accuracy drop. But success on clean benchmarks doesn’t guarantee usefulness in live systems.</li> <li><strong>Efficiency</strong>: Very few papers report runtime or resource costs. Yet many attacks (especially heuristic ones) could be too slow to be practical.</li> <li><strong>Transferability</strong>: Most attacks target a specific model. Few check if adversarial examples work across architectures, which is key in black-box settings.</li> <li><strong>Feasibility</strong>: Some attacks impose practical constraints (e.g., value ranges or valid encodings), but many assume idealized settings, ignoring feature accessibility, feature dependencies, or domain-specific rules.</li> <li><strong>Semantic Preservation</strong>: Does the modified input still make sense? A technically valid change (e.g., flipping “employed” to “unemployed”) may pass unnoticed by a model, but could violate the underlying meaning or ground truth of the example.</li> <li><strong>Plausibility</strong>: Would a human reviewing the input find it believable? In computer vision this is important, but only a few studies on tabular ML check for it. Some even argue plausibility may not be relevant for tabular attacks, reflecting an ongoing debate in the field.</li> <li><strong>Defense Awareness</strong>: Attacks are often tested on unprotected models. In reality, defenses like input validation, adversarial training, and monitoring are common.</li> <li><strong>Dataset Suitability</strong>: Too many studies rely on small, outdated academic datasets (like Iris or Wine) that don’t reflect real-world complexity or risk.</li> </ul> <p>Most papers addressed efficacy, and some considered feasibility. But beyond those, few went further. Most studies covered just two to six of the eight practical criteria, with four being the most common.</p> <h2 id="what-we-learned">What We Learned</h2> <p>Adversarial attacks on tabular models aren’t just academic exercises. In fields like finance, healthcare, and cybersecurity, these vulnerabilities can have real-world consequences. And yet, the field remains immature.</p> <p>We found innovative techniques tested only in idealized settings, with limited practical relevance. Many papers measure success as a drop in model accuracy, but rarely ask deeper questions: Is the attack realistic? Is the input plausible? Would it even be accepted by the system?</p> <p>We also found a lack of shared standards. Key concepts like semantic preservation and plausibility are used inconsistently and often left undefined. Code is frequently missing. Benchmarks are almost non-existent. And most attacks are tested on just one dataset and undefended models, making comparisons and cumulative progress difficult.</p> <h2 id="where-the-field-goes-next">Where the Field Goes Next</h2> <p>Despite these issues, the space for meaningful research in this domain remains open, especially as paradigms like pretraining, multitask learning, and tabular foundation models continue to emerge. What’s needed now isn’t just more attacks, but a stronger research foundation:</p> <ul> <li>Clear, shared definitions of robustness in tabular ML</li> <li>Benchmarks that reflect real-world constraints</li> <li>Broader, more realistic datasets</li> <li>Reproducible code and cross-domain evaluations</li> </ul> <p>If we want machine learning to be secure and trustworthy in critical systems, from credit scoring to malware detection, we need to stop treating tabular data as an afterthought.</p> <p>It’s time to give it the attention, clarity, and care it deserves.</p>]]></content><author><name></name></author><category term="research"/><category term="adversarial machine learning"/><category term="tabular data"/><category term="AI/ML security"/><summary type="html"><![CDATA[What happens when the most common data format in ML is also the least protected?]]></summary></entry></feed>